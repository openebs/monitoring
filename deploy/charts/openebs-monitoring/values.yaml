
# Default values for openebs-monitoring.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

## Provide a name in place of openebs-monitoring for `app:` labels
##
nameOverride: ""

## Provide a name to substitute for the full names of resources
##
fullnameOverride: ""

replicaCount: 1

#image:
  #repository: nginx
  #pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  #tag: ""

#imagePullSecrets: []

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

#service:
  # type: ClusterIP
  # port: 80

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths: []
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

## Configuration for kube-prometheus-stack subchart
kube-prometheus-stack:
  ## Create default rules for monitoring the cluster
  ##
  defaultRules:
    create: true
    rules:
      alertmanager: true
      etcd: true
      general: true
      k8s: true
      kubeApiserver: true
      kubeApiserverAvailability: true
      kubeApiserverError: true
      kubeApiserverSlos: true
      kubelet: true
      kubePrometheusGeneral: true
      kubePrometheusNodeAlerting: true
      kubePrometheusNodeRecording: true
      kubernetesAbsent: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      kubeScheduler: true
      kubeStateMetrics: true
      network: true
      node: true
      prometheus: true
      prometheusOperator: true
      time: true

  ## Provide custom recording or alerting rules to be deployed into the cluster.
  ##
  additionalPrometheusRulesMap: {}
  #  rule-name:
  #    groups:
  #    - name: my_group
  #      rules:
  #      - record: my_record
  #        expr: 100 * my_record

  ## Configuration for alertmanager
  ## ref: https://prometheus.io/docs/alerting/alertmanager/
  ##

  alertmanager:
    ## Deploy alertmanager
    ##
    enabled: true

    ## If true, create a serviceMonitor for alertmanager
    ##
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""
      selfMonitor: true

    ## Settings affecting alertmanagerSpec
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec
    ##
    alertmanagerSpec:
      ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.
      ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.
      ##
      configMaps: []

      ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for
      ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config.
      ##
      # configSecret:

      ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.
      ##
      alertmanagerConfigSelector: {}
      ## Example which selects all alertmanagerConfig resources
      ## with label "alertconfig" with values any of "example-config" or "example-config-2"
      # alertmanagerConfigSelector:
      #   matchExpressions:
      #     - key: alertconfig
      #       operator: In
      #       values:
      #         - example-config
      #         - example-config-2
      #
      ## Example which selects all alertmanagerConfig resources with label "role" set to "example-config"
      # alertmanagerConfigSelector:
      #   matchLabels:
      #     role: example-config

      ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.
      ##
      alertmanagerConfigNamespaceSelector: {}
      ## Example which selects all namespaces
      ## with label "alertmanagerconfig" with values any of "example-namespace" or "example-namespace-2"
      # alertmanagerConfigNamespaceSelector:
      #   matchExpressions:
      #     - key: alertmanagerconfig
      #       operator: In
      #       values:
      #         - example-namespace
      #         - example-namespace-2

      ## Example which selects all namespaces with label "alertmanagerconfig" set to "enabled"
      # alertmanagerConfigNamespaceSelector:
      #   matchLabels:
      #     alertmanagerconfig: enabled

  
  prometheus:
    enabled: true
    ## Annotations for Prometheus
    ##
    annotations: {}

    serviceMonitor:
      selfMonitor: true
    
    prometheusSpec:
      ## Define which Nodes the Pods are scheduled on.
      ## ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}
      
      ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
      ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.
      ##
      configMaps: []

      ## Namespaces to be selected for PrometheusRules discovery.
      ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.
      ## See https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
      ##
      ruleNamespaceSelector: {}

      ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the PrometheusRule resources created
      ##
      ruleSelectorNilUsesHelmValues: true

      ## PrometheusRules to be selected for target discovery.
      ## If {}, select all PrometheusRules
      ##
      ruleSelector: {}
      ## Example which select all PrometheusRules resources
      ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
      # ruleSelector:
      #   matchExpressions:
      #     - key: prometheus
      #       operator: In
      #       values:
      #         - example-rules
      #         - example-rules-2
      #
      ## Example which select all PrometheusRules resources with label "role" set to "example-rules"
      # ruleSelector:
      #   matchLabels:
      #     role: example-rules

      ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the servicemonitors created
      ##
      # false value prevents from adding any Helm label to serviceMonitorSelector if
      # above is empty.
      serviceMonitorSelectorNilUsesHelmValues: false

      ## ServiceMonitors to be selected for target discovery.
      ## If {}, select all ServiceMonitors
      ##
      serviceMonitorSelector: {}
      ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
      # serviceMonitorSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## Namespaces to be selected for ServiceMonitor discovery.
      ##
      serviceMonitorNamespaceSelector: {}
      ## Example which selects ServiceMonitors in namespaces with label "prometheus" set to "somelabel"
      # serviceMonitorNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the podmonitors created
      ##
      podMonitorSelectorNilUsesHelmValues: false

      ## PodMonitors to be selected for target discovery.
      ## If {}, select all PodMonitors
      ##
      podMonitorSelector: {}
      ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
      # podMonitorSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## Namespaces to be selected for PodMonitor discovery.
      ## See https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
      ##
      podMonitorNamespaceSelector: {}

      ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the probes created
      ##
      probeSelectorNilUsesHelmValues: true

      ## Probes to be selected for target discovery.
      ## If {}, select all Probes
      ##
      probeSelector: {}
      ## Example which selects Probes with label "prometheus" set to "somelabel"
      # probeSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## Namespaces to be selected for Probe discovery.
      ## See https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
      ##
      probeNamespaceSelector: {}

      ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
      ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
      ## as specified in the official Prometheus documentation:
      ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
      ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
      ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
      ## scrape configs are going to break Prometheus after the upgrade.
      ##
      ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
      ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
      ##
      additionalScrapeConfigs: []
      # - job_name: kube-etcd
      #   kubernetes_sd_configs:
      #     - role: node
      #   scheme: https
      #   tls_config:
      #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca
      #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client
      #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
      #   relabel_configs:
      #   - action: labelmap
      #     regex: __meta_kubernetes_node_label_(.+)
      #   - source_labels: [__address__]
      #     action: replace
      #     targetLabel: __address__
      #     regex: ([^:;]+):(\d+)
      #     replacement: ${1}:2379
      #   - source_labels: [__meta_kubernetes_node_name]
      #     action: keep
      #     regex: .*mst.*
      #   - source_labels: [__meta_kubernetes_node_name]
      #     action: replace
      #     targetLabel: node
      #     regex: (.*)
      #     replacement: ${1}
      #   metric_relabel_configs:
      #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)
      #     action: labeldrop

      ## If additional scrape configurations are already deployed in a single secret file you can use this section.
      ## Expected values are the secret name and key
      ## Cannot be used with additionalScrapeConfigs
      additionalScrapeConfigsSecret: {}
        # enabled: false
        # name:
        # key:

      ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful
      ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'
      additionalPrometheusSecretsAnnotations: {}

      ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified
      ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<alertmanager_config>.
      ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.
      ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this
      ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release
      ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.
      ##
      additionalAlertManagerConfigs: []
      # - consul_sd_configs:
      #   - server: consul.dev.test:8500
      #     scheme: http
      #     datacenter: dev
      #     tag_separator: ','
      #     services:
      #       - metrics-prometheus-alertmanager

      ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended
      ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the
      ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.
      ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the
      ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel
      ## configs are going to break Prometheus after the upgrade.
      ##
      additionalAlertRelabelConfigs: []
      # - separator: ;
      #   regex: prometheus_replica
      #   replacement: $1
      #   action: labeldrop

    additionalRulesForClusterRole: []
    #  - apiGroups: [ "" ]
    #    resources:
    #      - nodes/proxy
    #    verbs: [ "get", "list", "watch" ]

    additionalServiceMonitors: []

    additionalPodMonitors: []

  grafana:
    enabled: true
    ## Deploy default dashboards.
    ##
    defaultDashboardsEnabled: true
    adminPassword: admin
    sidecar:
      dashboards:
        enabled: true
        # ConfigMaps with label below will be added to Grafana as dashboards.
        label: grafana_dashboard
        ## Annotations for Grafana dashboard configmaps
        ##
        annotations: {}

        datasources:
          enabled: true
          defaultDatasourceEnabled: true

          # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default
          # defaultDatasourceScrapeInterval: 15s

          ## Annotations for Grafana datasource configmaps
          ##
          annotations: {}
          ## Create datasource for each Pod of Prometheus StatefulSet;
          ## this uses headless service `prometheus-operated` which is
          ## created by Prometheus Operator
          ## ref: https://git.io/fjaBS
          createPrometheusReplicasDatasources: false
          label: grafana_datasource
    
    extraConfigmapMounts: []
    # - name: certs-configmap
    #   mountPath: /etc/grafana/ssl/
    #   configMap: certs-configmap
    #   readOnly: true

    ## Configure additional grafana datasources (passed through tpl)
    ## ref: http://docs.grafana.org/administration/provisioning/#datasources
    additionalDataSources: []
    # - name: prometheus-sample
    #   access: proxy
    #   basicAuth: true
    #   basicAuthPassword: pass
    #   basicAuthUser: daco
    #   editable: false
    #   jsonData:
    #       tlsSkipVerify: true
    #   orgId: 1
    #   type: prometheus
    #   url: https://{{ printf "%s-prometheus.svc" .Release.Name }}:9090
    #   version: 1

  ## Component scraping the kube api server
  ##
  kubeApiServer:
    enabled: true

  ## Component scraping the kubelet and kubelet-hosted cAdvisor
  ##
  kubelet:
    enabled: true
    namespace: kube-system

  ## Component scraping the kube controller manager
  ##
  kubeControllerManager:
    enabled: true
    
    serviceMonitor:
      enabled: true

  ## Component scraping coreDns. Use either this or kubeDns
  ##
  coreDns:
    enabled: true

  ## Component scraping kubeDns. Use either this or coreDns
  ##
  kubeDns:
    enabled: false

  ## Component scraping etcd
  ##
  kubeEtcd:
    enabled: true
    
    serviceMonitor:
      enabled: true

  ## Component scraping kube scheduler
  ##
  kubeScheduler:
    enabled: true

    serviceMonitor:
      enabled: true

  ## Component scraping kube proxy
  ##
  kubeProxy:
    enabled: true
    
    ## If your kube proxy is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints: []
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24
    serviceMonitor:
      enabled: true

  ## Component scraping kube state metrics
  ##
  kubeStateMetrics:
    enabled: true

  ## Configuration for kube-state-metrics subchart
  ##
  kube-state-metrics:
    namespaceOverride: ""
    rbac:
      create: true
    podSecurityPolicy:
      enabled: true

  ## Deploy node exporter as a daemonset to all nodes
  ##
  nodeExporter:
    enabled: true

  ## Configuration for prometheus-node-exporter subchart
  ##
  prometheus-node-exporter:
    namespaceOverride: ""
    podLabels:
      ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
      ##
      jobLabel: node-exporter
    extraArgs:
      - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$



cstor:
  enabled: true

  cstorVolume:
    # enabled: true
    serviceMonitor:
      selfMonitor: true
      path: "/metrics"

    cstorVolumeService:
      portName: exporter

  cstorPool:
    # enabled: true
    podMonitor:
      selfMonitor: true
      path: "/metrics"
      
    cstorPoolPod:
      targetPort: 9500